{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day Trading Model - Local Notebook\n",
        "\n",
        "This notebook trains and evaluates the intraday classification model using Yahoo Finance 1-minute data. Run the notebook locally to experiment with parameters, inspect the dataset, and simulate the model's live signal output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment setup\n",
        "\n",
        "1. Create a virtual environment and activate it.\n",
        "2. Install dependencies with `pip install -r requirements.txt`.\n",
        "3. (Optional) Export broker environment variables if you plan to hit the Alpaca paper trading API:\n",
        "   ```bash\n",
        "   export BROKER_API_KEY=your-key\n",
        "   export BROKER_API_SECRET=your-secret\n",
        "   export BROKER_BASE_URL=https://paper-api.alpaca.markets\n",
        "   ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure the project package is on the path\n",
        "sys.path.append(str(Path.cwd()))\n",
        "\n",
        "from trading_models.config import AppConfig\n",
        "from trading_models.models.day_trading.config import DayTradingConfig\n",
        "from trading_models.models.day_trading.data import load_or_download, describe_data\n",
        "from trading_models.models.day_trading.features import engineer_features\n",
        "from trading_models.models.day_trading.pipeline import DayTradingPipeline\n",
        "from trading_models.models.day_trading.realtime import DayTradingStreamer\n",
        "from trading_models.utils import save_json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "app_cfg = AppConfig()\n",
        "app_cfg.ensure_directories()\n",
        "model_cfg = DayTradingConfig(symbol=\"AAPL\", lookback_days=10, epochs=12)\n",
        "model_cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download intraday dataset\n",
        "\n",
        "We fetch 1-minute candles from Yahoo Finance using the [`yfinance`](https://github.com/ranaroussi/yfinance) library. Data is cached inside `data/day_trading` so subsequent runs are faster unless you set `force=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "raw_df = load_or_download(app_cfg, model_cfg, force=False)\n",
        "describe_data(raw_df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "raw_df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature engineering\n",
        "\n",
        "We derive common technical indicators (simple/exponential moving averages, volatility, momentum, RSI) and create a binary target that indicates whether the next bar's return is greater than `threshold` (default 0.05%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "features_df, feature_cols = engineer_features(raw_df, model_cfg)\n",
        "features_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "len(features_df), len(feature_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/validation split\n",
        "\n",
        "The split is chronological to respect the time-series nature of intraday candles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "split_idx = int(len(features_df) * (1 - model_cfg.validation_size))\n",
        "split_idx = max(1, min(len(features_df) - 1, split_idx))\n",
        "train_df = features_df.iloc[:split_idx]\n",
        "val_df = features_df.iloc[split_idx:]\n",
        "\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df[\"target\"].values\n",
        "X_val = val_df[feature_cols].values\n",
        "y_val = val_df[\"target\"].values\n",
        "\n",
        "split_idx, X_train.shape, X_val.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the SGDClassifier model\n",
        "\n",
        "We fit an `SGDClassifier` with `log_loss` to allow `partial_fit` training epochs. The scaler and estimator are persisted to `artifacts/day_trading` for reuse by the web application and the real-time streamer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipeline = DayTradingPipeline(app_cfg, model_cfg)\n",
        "pipeline.model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "history = pipeline.model.fit(X_train, y_train, X_val, y_val)\n",
        "val_metrics = pipeline.model.evaluate(X_val, y_val)\n",
        "history, val_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot epoch metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.style.use('dark_background')\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "for key in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "    ax.plot([entry['epoch'] for entry in history], [entry[key] for entry in history], label=key)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Validation metrics per epoch')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion matrix on the validation window"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4, 4))\n",
        "y_pred_val = pipeline.model.predict(X_val)\n",
        "ConfusionMatrixDisplay.from_predictions(y_val, y_pred_val, ax=ax, cmap='Blues')\n",
        "ax.set_title('Validation confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Persist model artifacts and metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipeline.model.save()\n",
        "pipeline.storage_dir, list(pipeline.storage_dir.iterdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save metrics via the pipeline helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipeline_metadata = {\n",
        "    \"evaluation\": val_metrics,\n",
        "    \"history\": history,\n",
        "    \"metadata\": {\n",
        "        \"config\": model_cfg.__dict__,\n",
        "        \"features\": feature_cols,\n",
        "    },\n",
        "}\n",
        "save_json(pipeline.storage_dir / model_cfg.metrics_filename, pipeline_metadata)\n",
        "pipeline_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate real-time predictions\n",
        "\n",
        "After training, we can instantiate the `DayTradingStreamer` to pull the most recent minute bars, compute features, and stream probability scores. In a production setup you would call `submit_market_order` from `trading_models.broker.alpaca_client` when the signal is above/below your trade thresholds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "streamer = DayTradingStreamer(pipeline)\n",
        "stream_df = streamer.latest_points()\n",
        "stream_df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot latest prediction probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "ax.plot(stream_df['timestamp'], stream_df['probability'], label='Long probability', color='#facc15')\n",
        "ax.set_ylabel('Probability')\n",
        "ax.set_xlabel('Timestamp')\n",
        "ax.set_title('Streaming probability of positive return')\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "* Review and tune the feature windows, RSI period, and decision threshold to match your risk tolerance.\n",
        "* Integrate portfolio sizing, stop-loss and take-profit rules before sending orders to a live broker.\n",
        "* Schedule retraining using the CLI (`python -m trading_models.cli train day_trading`) or deploy the Flask app to Heroku using the included `Procfile`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}